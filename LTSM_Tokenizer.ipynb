{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from parse import get_data\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from parse import sample_data\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "train, labels = get_data(\"data/corpus.data\")\n",
    "nb_of_entries = len(train)\n",
    "print(nb_of_entries)\n",
    "df = pd.DataFrame({'Text' : train, 'Tag': labels})\n",
    "\n",
    "\n",
    "\n",
    "max_features = 100000\n",
    "tk = Tokenizer(num_words=max_features, split=' ')\n",
    "tk.fit_on_texts(df['Text'].values)\n",
    "X = tk.texts_to_sequences(df['Text'].values)\n",
    "# print(X)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "def result_predict(resultarray) :\n",
    "    if resultarray[0] == resultarray.max() :\n",
    "        return \"irr\"\n",
    "    if resultarray[1] == resultarray.max() :\n",
    "        return \"neg\"\n",
    "    if resultarray[2] == resultarray.max() :\n",
    "        return \"neu\"\n",
    "    if resultarray[3] == resultarray.max() :\n",
    "        return \"pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4498, 48) (4498, 4)\n",
      "(500, 48) (500, 4)\n",
      "Epoch 1/15\n",
      "4498/4498 [==============================] - 11s 3ms/step - loss: 1.2154 - acc: 0.4469\n",
      "Epoch 2/15\n",
      "4498/4498 [==============================] - 11s 3ms/step - loss: 1.0720 - acc: 0.5169\n",
      "Epoch 3/15\n",
      "4498/4498 [==============================] - 12s 3ms/step - loss: 0.9386 - acc: 0.6423\n",
      "Epoch 4/15\n",
      "4498/4498 [==============================] - 12s 3ms/step - loss: 0.7749 - acc: 0.6956\n",
      "Epoch 5/15\n",
      "4498/4498 [==============================] - 14s 3ms/step - loss: 0.6067 - acc: 0.7692\n",
      "Epoch 6/15\n",
      "4498/4498 [==============================] - 11s 3ms/step - loss: 0.4665 - acc: 0.8330\n",
      "Epoch 7/15\n",
      "4498/4498 [==============================] - 11s 3ms/step - loss: 0.3604 - acc: 0.8784\n",
      "Epoch 8/15\n",
      "4498/4498 [==============================] - 12s 3ms/step - loss: 0.2761 - acc: 0.9062\n",
      "Epoch 9/15\n",
      "4498/4498 [==============================] - 13s 3ms/step - loss: 0.2216 - acc: 0.9242\n",
      "Epoch 10/15\n",
      "4498/4498 [==============================] - 12s 3ms/step - loss: 0.1832 - acc: 0.9395\n",
      "Epoch 11/15\n",
      "4498/4498 [==============================] - 12s 3ms/step - loss: 0.1607 - acc: 0.9509\n",
      "Epoch 12/15\n",
      "4498/4498 [==============================] - 14s 3ms/step - loss: 0.1342 - acc: 0.9589\n",
      "Epoch 13/15\n",
      "4498/4498 [==============================] - 12s 3ms/step - loss: 0.1130 - acc: 0.9624\n",
      "Epoch 14/15\n",
      "4498/4498 [==============================] - 14s 3ms/step - loss: 0.1001 - acc: 0.9671\n",
      "Epoch 15/15\n",
      "4498/4498 [==============================] - 18s 4ms/step - loss: 0.0924 - acc: 0.9711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f4ba56940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# print(model.summary())\n",
    "\n",
    "Y = pd.get_dummies(df['Tag']).values\n",
    "percentage_train = 0.9\n",
    "borne = int(percentage_train*nb_of_entries)\n",
    "X_train = X[:borne]\n",
    "Y_train = Y[:borne]\n",
    "X_test = X[borne:]\n",
    "Y_test = Y[borne:]\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "batch_size = 200\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6082910482977093 0.5619335291608943\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(score, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revoir les taux votes par csp niveau revenu lors des législatives tant que les classesmoyennes sup sont pas lessivées comme les gilets jaunes par macron président des ultras riches lrem sera devant attendre ans désindexation retraite \n",
      "pos\n",
      "vous êtes une grande naïveté monsieur ensorcelé par gourou soudaine empathie pour ceux qui sont rien vous surprend pas cette période approchant les élections européennes macron essaie soigner son image arrogant hors sol \n",
      "pos\n",
      "macron fera jamais rien qui puisse choquer les musulmans les islamistes car est une part importante son électorat trahison \n",
      "pos\n",
      "dictature régime politique dans lequel tous les pouvoirs sont entre les mains une seule personne oui effectivement macron est pas seul ceux qui ont intérêt que reste comme aide bcp vrai décide pas chose \n",
      "pos\n",
      "affaire macron benalla réponse nos détracteurs via \n",
      "pos\n",
      "est macron qui est complice pire\n",
      "pos\n",
      "bref jsuis vener cause vos histoires plus macron fait des bottle flip pendant que samedi peux plus sortir chez moi parce que plus bus cause des gilets jaunes \n",
      "pos\n",
      "vision européenne macron est celle balladur bref ump chez mais après les élections une fois pouvoir frexit\n",
      "pos\n",
      "emmanuel macron veut instaurer une fiscalité taxe carbone liée aux cours mondiaux \n",
      "pos\n",
      "derrière les soutiens mlp surtout ceux qui appartiennent milieu journalistique général est pour sauver macron nous connaissons maintenant stratégie des médias \n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_to_test = sample_data(\"tw_db_prepared.data\", 10)\n",
    "\n",
    "for text in list_to_test :\n",
    "    print(text)\n",
    "    twt = tk.texts_to_sequences(text)\n",
    "    twt = pad_sequences(twt, maxlen=62, dtype='int32', value=0)\n",
    "    sentiment=model.predict(twt, batch_size=1, verbose=2)[0]\n",
    "    print(result_predict(sentiment))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    3   31    2   96 4727  254  877]]\n",
      "neg\n",
      "[0.11338186 0.544712   0.10707022 0.23483588]\n"
     ]
    }
   ],
   "source": [
    "twt = ['macron est le meilleur président, les autres candidats n\\'ont aucun chance']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tk.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=39, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "print(result_predict(sentiment))\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
